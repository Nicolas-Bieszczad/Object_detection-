# -*- coding: utf-8 -*-
"""Vnexia_Phase_2_V3.ipynb

Automatically generated by Colab.

## Imports and installations
"""

!pip install ultralytics

!pip install roboflow

!pip install wandb

!wandb login

# Install the required packages for Ultralytics YOLO and Weights & Biases
!pip install -U ultralytics wandb

# Enable W&B logging for Ultralytics
!yolo settings wandb=True

from roboflow import Roboflow
from ultralytics import YOLO
import wandb
import itertools
import os
import time
from pathlib import Path
import random
from shutil import copy2
import shutil
import cv2
import time
import torch
from wandb.integration.ultralytics import add_wandb_callback
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# Loading Dataset

### Dataset
"""

rf = Roboflow(api_key="ROBOFLOW_API_KEY")
project = rf.workspace("vnexia-internship").project("phase-2-worker-v2")
version = project.version(3)
dataset = version.download("yolov11")

"""### New 70/20/10 split"""

# Set original dataset path
dataset_path = Path("/content/Phase-2-worker-v2-3")
original_images_dir = dataset_path / "all" / "images"
original_labels_dir = dataset_path / "all" / "labels"

# Create `all/` folder to hold everything before splitting

(dataset_path / "all" / "images").mkdir(parents=True, exist_ok=True)
(dataset_path / "all" / "labels").mkdir(parents=True, exist_ok=True)

# Move current files to 'all/'
for sub in ["images", "labels"]:
    for f in (dataset_path / "train" / sub).glob("*"):
        f.rename(dataset_path / "all" / sub / f.name)

# Create split folders
splits = {
    "train": 0.7,
    "valid": 0.2,
    "test": 0.1
}

all_images = list(original_images_dir.glob("*.jpg"))
random.shuffle(all_images)

n = len(all_images)
train_cutoff = int(splits["train"] * n)
val_cutoff = train_cutoff + int(splits["valid"] * n)

split_data = {
    "train": all_images[:train_cutoff],
    "valid": all_images[train_cutoff:val_cutoff],
    "test": all_images[val_cutoff:]
}

# Copy to new split folders
for split, imgs in split_data.items():
    split_img_dir = dataset_path / split / "images"
    split_lbl_dir = dataset_path / split / "labels"
    split_img_dir.mkdir(parents=True, exist_ok=True)
    split_lbl_dir.mkdir(parents=True, exist_ok=True)

    for img_path in imgs:
        label_path = original_labels_dir / (img_path.stem + ".txt")
        if label_path.exists():
            copy2(img_path, split_img_dir / img_path.name)
            copy2(label_path, split_lbl_dir / label_path.name)

print("‚úÖ Dataset split into train/valid/test (70/20/10)")
shutil.rmtree("/content/Phase-2-worker-v2-3/all")

"""### Filtering dataset in order to keep only images with at least one object labeled 'person'"""

# Define dataset location
dataset_path = Path(dataset.location)

# Define splits to process
splits = ['train', 'valid', 'test']

# Loop over each split
for split in splits:
    labels_dir = dataset_path / split / "labels"
    images_dir = dataset_path / split / "images"

    kept_count = 0

    for label_file in labels_dir.glob("*.txt"):
        with open(label_file, "r") as f:
            lines = f.readlines()

        # Filter and relabel only class '3' (person -> worker)
        filtered_lines = []
        for line in lines:
            parts = line.strip().split()
            if parts and parts[0] == "3":
                parts[0] = "0"  # Rename to class 0 ("worker")
                filtered_lines.append(" ".join(parts) + "\n")

        if filtered_lines:
            # Save new labels
            kept_count += 1
            with open(label_file, "w") as f:
                f.writelines(filtered_lines)
        else:
            # Remove label file and corresponding image
            os.remove(label_file)
            image_file = images_dir / (label_file.stem + ".jpg")
            if image_file.exists():
                os.remove(image_file)

    print(f"‚úÖ [{split}] Kept {kept_count} images with at least one 'person' label.")

# Update data.yaml
yaml_path = dataset_path / "data.yaml"
with open(yaml_path, "w") as f:
    f.write("train: ../train/images\n")
    f.write("val: ../valid/images\n")
    f.write("test: ../test/images\n")
    f.write("nc: 1\n")
    f.write("names: ['worker']\n")

print("üìÅ All dataset splits filtered. 'person' class relabeled as 'worker'. Ready for training.")

"""### Training with gridsearch and wandb"""

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

sweep_config = {
    "method": "grid",
    "metric": {
        "name": "metrics/mAP_0.5",
        "goal": "maximize"
    },
    "parameters": {
        "epochs": {"values": [10]},
        "batch": {"values": [16,32]},
        "lr0": {"values": [1e-3, 1e-4]},
        "optimizer": {"values": ["SGD", "Adam"]}
    }
}

def train():
    run = wandb.init(
        project="yolo-worker-gridsearch",
        job_type='training'
    )
    config = run.config
    run_name = f"e{config.epochs}_b{config.batch}_lr{config.lr0}_{config.optimizer}"
    wandb.run.name = run_name

    model = YOLO("yolo11n.pt")
    add_wandb_callback(model, enable_model_checkpointing=True)

    model.train(
        data="/content/Phase-2-worker-v2-3/data.yaml",
        epochs=config.epochs,
        batch=config.batch,
        lr0=config.lr0,
        optimizer=config.optimizer,
        project="yolo-worker-gridsearch",  # SAME as sweep project
        name=run_name,
        device=0 if torch.cuda.is_available() else "cpu",
        val=True
    )


    run.finish()

sweep_id = wandb.sweep(sweep_config, project="yolo-worker-gridsearch")
wandb.agent(sweep_id, function=train)

"""### HeatMap"""

# Load CSV
df = pd.read_csv('/content/grid_search_results.csv')

# Get unique optimizers
optimizers = df['optimizer'].unique()

# Create subplots: 1 row, N columns (one per optimizer)
fig, axes = plt.subplots(1, len(optimizers), figsize=(8 * len(optimizers), 6))

if len(optimizers) == 1:
    axes = [axes]  # Make it iterable

for ax, optimizer in zip(axes, optimizers):
    df_opt = df[df['optimizer'] == optimizer]
    pivot_table = df_opt.pivot(index='batch', columns='lr0', values='metrics/mAP50(B)')

    sns.heatmap(pivot_table, annot=True, fmt=".3f", cmap="YlGnBu", ax=ax)
    ax.set_title(f"mAP@0.5 Heatmap ({optimizer})")
    ax.set_xlabel("Learning Rate (lr0)")
    ax.set_ylabel("Batch Size")

plt.tight_layout()
plt.show()

"""### Training with best params"""

model = YOLO('yolo11n.pt')
add_wandb_callback(model, enable_model_checkpointing=True)

model.train(
    data='/content/Phase-2-worker-v2-3/data.yaml',
    epochs=100,
    batch=32,
    lr0=1e-4,
    optimizer='Adam',
    project='worker-detection_optimized',
    name='e100_b32_lr0.0001_adam',
    device=0,
    val=True
)

"""### Test on test split"""

best_weights_path = "/content/drive/MyDrive/worker-detection_optimized/e100_b32_lr0.0001_adam2/weights/best.pt"
model = YOLO(best_weights_path)

# Evaluate on the test set
results = model.val(
    data='/content/Phase-2-worker-v2-3/data.yaml',
    split='test'
)

# Print summary metrics
print("Precision:", results.box.mp)
print("Recall:", results.box.mr)
print("mAP@0.5:", results.box.map50)
print("mAP@0.5:0.95:", results.box.map)